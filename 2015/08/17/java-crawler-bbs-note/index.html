<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Java爬虫爬取小百合BBS小记 · Mcl's space</title><meta name="description" content="Java爬虫爬取小百合BBS小记 - LinChen"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/rdmclin2" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/rdmclin2" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Java爬虫爬取小百合BBS小记</h1><div class="post-time">Aug 17, 2015</div><div class="post-content"><h1 id="前言">前言</h1><p>本文关于<a href="http://bbs.nju.edu.cn" target="_blank" rel="external">小百合论坛</a>的爬取，共458个板块，每个板块1000个的帖子，主要借助开源包<a href="http://sourceforge.net/projects/htmlparser/files/Integration-Builds/2.0-20060923/" target="_blank" rel="external">htmlparser</a>,下面上分析过程和代码。注意本文在一篇<a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-crawler/index.html" target="_blank" rel="external">ibm的文章</a>的基础上进行修改。</p>
<h2 id="更新">更新</h2><p>我另外简单实现了一个MapReduce版本的，不过实现的不好，权当参考一下吧.奉上<a href="git@github.com:rdmclin2/MR_BBSCrawler.git">github地址</a>，上面有简单介绍.</p>
<a id="more"></a>
<hr>
<h1 id="分析过程及代码">分析过程及代码</h1><h2 id="关于爬虫">关于爬虫</h2><p>爬虫大家都知道，由一个种子页(seed)出发，分析该页，获取页面链接，加入未访问的列表中，从未访问的表中取链接，访问，重复如上步骤即可，要注意的就是要维护一个已访问的网页列表，以避免重复访问。然而这种广义的爬虫并不符合特定的情况，例如我现在要爬的bbs板块帖子，遇到具体问题，还是得具体分析。</p>
<hr>
<h2 id="小百合bbs爬取过程">小百合bbs爬取过程</h2><p>基本的爬取步骤如下：</p>
<ul>
<li>首先获得小百合bbs的所有板块链接</li>
<li>对于每一个板块，获取至多1000条帖子的链接</li>
<li>对于每一条链接，获取帖子内容并存储</li>
</ul>
<hr>
<h3 id="获取所有板块链接">获取所有板块链接</h3><p>那么，最好有个入口页面可以把所有板块的链接都收集好，直接爬就好，幸运的是小百合贴心的有个叫做<a href="http://bbs.nju.edu.cn/bbsall" target="_blank" rel="external">全部讨论区</a>的版面,lucky~省下不少功夫.<br>这一部分的代码如下(BoardParser.java):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">package com.mcl.crawler;&#10;&#10;import java.util.HashSet;&#10;import java.util.Set;&#10;&#10;import org.htmlparser.Node;&#10;import org.htmlparser.Parser;&#10;import org.htmlparser.filters.NodeClassFilter;&#10;import org.htmlparser.tags.LinkTag;&#10;import org.htmlparser.util.NodeList;&#10;import org.htmlparser.util.ParserException;&#10;&#10;public class BoardParser &#123;&#10;&#9;// &#33719;&#21462;bbs&#32593;&#31449;&#19978;&#30340;&#25152;&#26377;&#26495;&#22359;&#38142;&#25509;&#10;&#9;private Set&#60;String&#62; extracLinks(String url, LinkFilter filter) &#123;&#10;&#10;&#9;&#9;Set&#60;String&#62; links = new HashSet&#60;String&#62;();&#10;&#9;&#9;try &#123;&#10;&#9;&#9;&#9;Parser parser = new Parser(url);&#10;&#9;&#9;&#9;parser.setEncoding(&#34;UTF-8&#34;);&#10;&#9;&#9;&#9;// linkFilter &#26469;&#35774;&#32622;&#36807;&#28388; &#60;a&#62; &#26631;&#31614;&#10;&#9;&#9;&#9;NodeClassFilter linkFilter = new NodeClassFilter(LinkTag.class);&#10;&#9;&#9;&#9;// &#24471;&#21040;&#25152;&#26377;&#32463;&#36807;&#36807;&#28388;&#30340;&#26631;&#31614;&#10;&#9;&#9;&#9;NodeList list = parser.extractAllNodesThatMatch(linkFilter);&#10;&#9;&#9;&#9;for (int i = 0; i &#60; list.size(); i++) &#123;&#10;&#9;&#9;&#9;&#9;Node tag = list.elementAt(i);&#10;&#9;&#9;&#9;&#9;if (tag instanceof LinkTag)// &#60;a&#62; &#26631;&#31614;&#10;&#9;&#9;&#9;&#9;&#123;&#10;&#9;&#9;&#9;&#9;&#9;LinkTag link = (LinkTag) tag;&#10;&#9;&#9;&#9;&#9;&#9;String linkUrl = link.getLink();// url&#10;&#9;&#9;&#9;&#9;&#9;if (filter.accept(linkUrl)) &#123;&#10;&#9;&#9;&#9;&#9;&#9;&#9;// &#23558;&#19968;&#33324;&#27169;&#24335;&#20999;&#25442;&#25104;&#20027;&#39064;&#27169;&#24335;&#10;&#9;&#9;&#9;&#9;&#9;&#9;String stlink = linkUrl.replace(&#34;bbsdoc&#34;, &#34;bbstdoc&#34;);&#10;&#9;&#9;&#9;&#9;&#9;&#9;links.add(stlink);&#10;&#9;&#9;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#125; catch (ParserException e) &#123;&#10;&#9;&#9;&#9;e.printStackTrace();&#10;&#9;&#9;&#125;&#10;&#9;&#9;return links;&#10;&#9;&#125;&#10;&#10;&#9;// &#33719;&#21462;&#26495;&#22359;&#38142;&#25509;&#24182;&#20999;&#25442;&#25104;&#20027;&#39064;&#27169;&#24335;&#10;&#9;public Set&#60;String&#62; getBoards(String seed) &#123;&#10;&#9;&#9;Set&#60;String&#62; links = extracLinks(seed, new LinkFilter() &#123;&#10;&#9;&#9;&#9;// &#25552;&#21462;&#20197; http://bbs.nju.edu.cn/bbsdoc &#24320;&#22836;&#30340;&#38142;&#25509;&#10;&#9;&#9;&#9;public boolean accept(String url) &#123;&#10;&#9;&#9;&#9;&#9;if (url.startsWith(&#34;http://bbs.nju.edu.cn/bbsdoc&#34;))&#10;&#9;&#9;&#9;&#9;&#9;return true;&#10;&#9;&#9;&#9;&#9;else&#10;&#9;&#9;&#9;&#9;&#9;return false;&#10;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#125;);&#10;&#9;&#9;// System.out.println(links.size());&#10;&#9;&#9;// for(String link : links)&#123;&#10;&#9;&#9;// System.out.println(link);&#10;&#9;&#9;// &#125;&#10;&#9;&#9;return links;&#10;&#9;&#125;&#10;&#10;&#9;// &#27979;&#35797;&#30340; main &#26041;&#27861;&#10;&#9;// public static void main(String[]args)&#123;&#10;&#9;// BoardParser.getBoards(&#34;http://bbs.nju.edu.cn/bbsall&#34;);&#10;&#9;// &#125;&#10;&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="对于每一个板块，获取至多1000条帖子的链接">对于每一个板块，获取至多1000条帖子的链接</h3><p>然后有了页面的链接，就要考虑怎么获取1000个帖子，我们并不想爬取回复，直接爬取主题，因此你可以看到上面将链接中的bbsdoc换成了bbstdoc，这就是主题模式。注意到bbs有个<code>上一页</code>的链接，那么获取这个链接，可以获得更多的帖子，不断反复，直到获取1000个帖子或没有更多的帖子.<br>以下是主要处理代码(FileParser.java):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">package com.mcl.crawler;&#10;&#10;import java.util.HashMap;&#10;import java.util.Map;&#10;import java.util.Map.Entry;&#10;&#10;import org.htmlparser.Node;&#10;import org.htmlparser.Parser;&#10;import org.htmlparser.filters.NodeClassFilter;&#10;import org.htmlparser.tags.LinkTag;&#10;import org.htmlparser.util.NodeList;&#10;import org.htmlparser.util.ParserException;&#10;&#10;//&#33719;&#21462;bbs&#32593;&#31449;&#19978;&#26576;&#26495;&#22359;&#30340;&#25152;&#26377;&#24086;&#23376;&#38142;&#25509;&#10;public class FileParser &#123;&#10;&#9;//&#27599;&#20010;&#26495;&#22359;&#29228;&#21462;1000&#20010;&#24038;&#21491;&#30340;&#24086;&#23376;&#10;&#9;private final int LINK_UP = 1000;&#10;&#10;&#9;Map&#60;String,String&#62; links = new HashMap&#60;String,String&#62;();&#10;&#9;private  String extracLinks(String url,LinkFilter filter) &#123;&#10;&#9;&#9;//&#19979;&#19968;&#39029;&#30340;&#38142;&#25509;&#10;&#9;&#9;String nextPage = null;&#10;&#9;&#9;try &#123;&#10;&#9;&#9;&#9;Parser parser = new Parser(url);&#10;&#9;&#9;&#9;parser.setEncoding(&#34;gb2312&#34;);&#10;&#9;&#9;&#9;// linkFilter &#26469;&#35774;&#32622;&#36807;&#28388; &#60;a&#62; &#26631;&#31614;&#10;&#9;&#9;&#9;NodeClassFilter linkFilter = new NodeClassFilter(&#10;&#9;&#9;&#9;&#9;&#9;LinkTag.class);&#10;&#9;&#9;&#9;// &#24471;&#21040;&#25152;&#26377;&#32463;&#36807;&#36807;&#28388;&#30340;&#26631;&#31614;&#10;&#9;&#9;&#9;NodeList list = parser.extractAllNodesThatMatch(linkFilter);&#10;&#9;&#9;&#9;for (int i = 0; i &#60; list.size(); i++) &#123;&#10;&#9;&#9;&#9;&#9;Node tag = list.elementAt(i);&#10;&#9;&#9;&#9;&#9;if (tag instanceof LinkTag)// &#60;a&#62; &#26631;&#31614;&#10;&#9;&#9;&#9;&#9;&#123;&#10;&#9;&#9;&#9;&#9;&#9;LinkTag link = (LinkTag) tag;&#10;&#9;&#9;&#9;&#9;&#9;String linkUrl = link.getLink();// url&#10;&#10;&#9;&#9;&#9;&#9;&#9;//&#33719;&#21462;&#19978;&#19968;&#39029;&#30340;&#38142;&#25509;&#65292;&#20316;&#20026;&#19979;&#19968;&#27493;&#30340;&#38142;&#25509;&#10;&#9;&#9;&#9;&#9;&#9;if(link.getLinkText().equals(&#34;&#19978;&#19968;&#39029;&#34;))&#123;&#10;&#9;&#9;&#9;&#9;&#9;&#9;nextPage = linkUrl;&#10;&#9;&#9;&#9;&#9;&#9;&#125;&#10;&#10;&#9;&#9;&#9;&#9;&#9;if(filter.accept(linkUrl) &#38;&#38; links.size() &#60; LINK_UP)&#123;&#10;&#9;&#9;&#9;&#9;&#9;&#9;String title = link.getLinkText();&#10;&#9;&#9;&#9;&#9;&#9;&#9;links.put(linkUrl, title);&#10;&#9;&#9;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#125; catch (ParserException e) &#123;&#10;&#9;&#9;&#9;e.printStackTrace();&#10;&#9;&#9;&#125;&#10;&#9;&#9;return nextPage;&#10;&#9;&#125;&#10;&#10;&#10;&#9;&#9;//&#33719;&#21462;&#26495;&#22359;&#38142;&#25509;&#24182;&#20999;&#25442;&#25104;&#20027;&#39064;&#27169;&#24335;&#10;&#9;&#9;public  Map&#60;String, String&#62; getFileUrls(String seed)&#10;&#9;&#9;&#123;&#10;&#9;&#9;&#9;System.out.println(&#34;seed: &#34;+ seed);&#10;&#9;&#9;&#9;links.clear();&#10;&#9;&#9;&#9;int linkCount = 0;&#10;&#9;&#9;&#9;final String rule = seed.replace(&#34;bbstdoc&#34;, &#34;bbstcon&#34;);&#10;&#9;&#9;&#9;String list = seed;&#10;&#9;&#9;&#9;while(linkCount &#60; LINK_UP)&#123;&#10;&#9;&#9;&#9;&#9;list = extracLinks(list,new LinkFilter()&#10;&#9;&#9;&#9;&#9;&#123;&#10;&#9;&#9;&#9;&#9;&#9;//&#25552;&#21462;&#20197; http://bbs.nju.edu.cn/bbstcon?board=... &#24320;&#22836;&#30340;&#38142;&#25509;&#10;&#9;&#9;&#9;&#9;&#9;public boolean accept(String url) &#123;&#10;&#9;&#9;&#9;&#9;&#9;&#9;if(url.startsWith(rule))&#10;&#9;&#9;&#9;&#9;&#9;&#9;&#9;return true;&#10;&#9;&#9;&#9;&#9;&#9;&#9;else&#10;&#9;&#9;&#9;&#9;&#9;&#9;&#9;return false;&#10;&#9;&#9;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;&#9;&#125;);&#10;&#9;&#9;&#9;&#9;linkCount = links.size();&#10;&#9;&#9;&#9;&#9;//&#22914;&#26524;&#27809;&#26377;&#19979;&#19968;&#39029;&#30340;&#38142;&#25509;&#20102;&#10;&#9;&#9;&#9;&#9;if(list == null)&#123;&#10;&#9;&#9;&#9;&#9;&#9;break;&#10;&#9;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;&#9;try &#123;&#10;&#9;&#9;&#9;&#9;&#9;Thread.sleep(360);&#10;&#9;&#9;&#9;&#9;&#125; catch (InterruptedException e) &#123;&#10;&#9;&#9;&#9;&#9;&#9;// TODO Auto-generated catch block&#10;&#9;&#9;&#9;&#9;&#9;e.printStackTrace();&#10;&#9;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;System.out.println(&#34;&#20849; &#34;+ links.size()+&#34; &#31687;&#24086;&#23376;&#34;);&#10;&#9;&#9;&#9;for (Entry&#60;String, String&#62; entry : links.entrySet()) &#123;&#10;&#9;&#9;&#9;&#9;System.out.println(entry.getKey()+&#34; &#34;+entry.getValue());&#10;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;return links;&#10;&#9;&#9;&#125;&#10;&#10;&#9;&#9;//&#27979;&#35797;&#30340; main &#26041;&#27861;&#10;&#9;&#9;public static void main(String[]args)&#123;&#10;&#9;&#9;&#9;FileParser parser = new FileParser();&#10;&#9;&#9;&#9;parser.getFileUrls(&#34;http://bbs.nju.edu.cn/bbstdoc?board=Blog&#34;);&#10;&#9;&#9;&#125;&#10;&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="对于每一条链接，获取帖子内容并存储">对于每一条链接，获取帖子内容并存储</h3><p>得到帖子的链接，下面就是获取帖子的内容,我们获取table-&gt;tbody-&gt;第二个tr-&gt;td-&gt;pre中的纯文本。<br>代码如下(FileDownLoader):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">package com.mcl.crawler;&#10;&#10;import java.io.BufferedWriter;&#10;import java.io.File;&#10;import java.io.FileOutputStream;&#10;import java.io.IOException;&#10;import java.io.OutputStreamWriter;&#10;&#10;import org.htmlparser.NodeFilter;&#10;import org.htmlparser.Parser;&#10;import org.htmlparser.filters.AndFilter;&#10;import org.htmlparser.filters.HasAttributeFilter;&#10;import org.htmlparser.filters.NodeClassFilter;&#10;import org.htmlparser.filters.TagNameFilter;&#10;import org.htmlparser.tags.TableTag;&#10;import org.htmlparser.util.NodeList;&#10;import org.htmlparser.util.ParserException;&#10;&#10;public class FileDownLoader &#123;&#10;public static boolean createFile(String destFileName) &#123;  &#10;      File file = new File(destFileName);  &#10;      if(file.exists()) &#123;  &#10;          System.out.println(&#34;&#21019;&#24314;&#21333;&#20010;&#25991;&#20214;&#34; + destFileName + &#34;&#22833;&#36133;&#65292;&#30446;&#26631;&#25991;&#20214;&#24050;&#23384;&#22312;&#65281;&#34;);&#10;          return false;  &#10;      &#125;  &#10;      if (destFileName.endsWith(File.separator)) &#123;  &#10;          System.out.println(&#34;&#21019;&#24314;&#21333;&#20010;&#25991;&#20214;&#34; + destFileName + &#34;&#22833;&#36133;&#65292;&#30446;&#26631;&#25991;&#20214;&#19981;&#33021;&#20026;&#30446;&#24405;&#65281;&#34;);  &#10;          return false;  &#10;      &#125;  &#10;      //&#21028;&#26029;&#30446;&#26631;&#25991;&#20214;&#25152;&#22312;&#30340;&#30446;&#24405;&#26159;&#21542;&#23384;&#22312;  &#10;      if(!file.getParentFile().exists()) &#123;  &#10;          //&#22914;&#26524;&#30446;&#26631;&#25991;&#20214;&#25152;&#22312;&#30340;&#30446;&#24405;&#19981;&#23384;&#22312;&#65292;&#21017;&#21019;&#24314;&#29238;&#30446;&#24405;  &#10;          //System.out.println(&#34;&#30446;&#26631;&#25991;&#20214;&#25152;&#22312;&#30446;&#24405;&#19981;&#23384;&#22312;&#65292;&#20934;&#22791;&#21019;&#24314;&#23427;&#65281;&#34;);  &#10;          if(!file.getParentFile().mkdirs()) &#123;  &#10;              System.out.println(&#34;&#21019;&#24314;&#30446;&#26631;&#25991;&#20214;&#25152;&#22312;&#30446;&#24405;&#22833;&#36133;&#65281;&#34;);  &#10;              return false;  &#10;          &#125;  &#10;      &#125;  &#10;      //&#21019;&#24314;&#30446;&#26631;&#25991;&#20214;  &#10;      try &#123;  &#10;          if (file.createNewFile()) &#123;  &#10;              return true;  &#10;          &#125; else &#123;  &#10;              return false;  &#10;          &#125;  &#10;      &#125; catch (IOException e) &#123;  &#10;          e.printStackTrace();  &#10;          System.out.println(&#34;&#21019;&#24314;&#21333;&#20010;&#25991;&#20214;&#34; + destFileName + &#34;&#22833;&#36133;&#65281;&#34; + e.getMessage());  &#10;          return false;  &#10;      &#125;  &#10;  &#125;&#10;&#10;&#10;/**&#20445;&#23384;&#32593;&#39029;&#23383;&#33410;&#25968;&#32452;&#21040;&#26412;&#22320;&#25991;&#20214;&#10; * filePath &#20026;&#35201;&#20445;&#23384;&#30340;&#25991;&#20214;&#30340;&#30456;&#23545;&#22320;&#22336;&#10; */&#10; private void saveToLocal(String content,String filePath)&#10; &#123;&#10;   try &#123;&#10;     createFile(filePath);&#10;     OutputStreamWriter osw;&#10;     String encoding = &#34;UTF-8&#34;;&#10;     osw = new OutputStreamWriter(&#10;         new FileOutputStream(filePath), encoding);&#10;     BufferedWriter bw = new BufferedWriter(osw);&#10;     bw.write(content);&#10;&#10;     bw.close();&#10;     osw.close();&#10;   &#125; catch (IOException e) &#123;&#10;     e.printStackTrace();&#10;   &#125;&#10; &#125;&#10;&#10;/*&#19979;&#36733; url &#25351;&#21521;&#30340;&#32593;&#39029;&#30340;&#24086;&#23376;&#20869;&#23481;&#24182;&#23384;&#20648;*/&#10;public String  downloadFile(String url,String filePath)&#10;&#123;&#10;  System.out.println(&#34;filePath: &#34;+filePath);&#10;  Parser parser;&#10;  try &#123;&#10;    parser = new Parser(url);&#10;    parser.setEncoding(&#34;UTF-8&#34;);&#10;    NodeList tableOfPre1 = parser.extractAllNodesThatMatch(&#10;          (NodeFilter) new AndFilter(new NodeClassFilter(TableTag.class), new HasAttributeFilter(&#34;class&#34;, &#34;main&#34;)));&#10;      if(tableOfPre1 != null &#38;&#38; tableOfPre1.size() &#62; 0) &#123;&#10;         // &#33719;&#21462;&#25351;&#23450; table &#26631;&#31614;&#30340;&#23376;&#33410;&#28857;&#20013;&#30340; &#60;tbody&#62; &#33410;&#28857;&#10;         NodeList tList = tableOfPre1.elementAt(0).getChildren().extractAllNodesThatMatch (new TagNameFilter(&#34;tr&#34;), true);&#10;         //&#31532;&#20108;&#21015;&#31532;&#19968;&#20010;tag&#65306;pre&#10;         String text = tList.elementAt(1).getFirstChild().toPlainTextString();&#10;         //System.out.println(text);&#10;         saveToLocal(text,filePath);&#10;&#10;      &#125;&#10;    &#125; catch (ParserException e) &#123;&#10;      e.printStackTrace();&#10;    &#125;&#10;  return null;&#10;&#125;&#10;//&#27979;&#35797;&#30340; main &#26041;&#27861;&#10;public static void main(String[]args)&#10;&#123;&#10;  FileDownLoader downLoader = new FileDownLoader();&#10;  downLoader.downloadFile(&#34;http://bbs.nju.edu.cn/bbstcon?board=Blog&#38;file=M.1373384270.A&#34;,&#34;temp/Blog/&#25105;&#31169;&#20154;&#21306;&#26368;&#21518;&#19968;&#31687;&#25991;&#31456;&#33258;&#21160;&#28040;&#22833;&#20102;&#65292;&#33021;&#25214;&#22238;&#21527;&#65311;.txt&#34;);&#10;&#125;&#10;&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<h3 id="main文件">main文件</h3><p>最后把这些步骤串起来,分为3层进行处理,因为帖子名称可能重复，因此在生成的文件名中加入帖子的id以保证唯一性,最后因为小百合连续获取帖子会出错，因此我们间隔一段时间获取数据，这个时间需要自己测试.<br>主要代码如下(Crawler.java)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">package com.mcl.crawler;&#10;&#10;import java.util.Dictionary;&#10;import java.util.Map;&#10;import java.util.Set;&#10;import java.util.Map.Entry;&#10;public class Crawler &#123;&#10;&#9;private final String dir = &#34;temp&#34;;&#10;&#9;/* &#29228;&#21462;&#26041;&#27861;*/&#10;&#9;public void crawling(String seed)&#10;&#9;&#123;&#10;&#9;&#9;FileParser fileParser = new FileParser();&#10;&#9;&#9;BoardParser boardParser = new BoardParser();&#10;&#9;&#9;FileDownLoader downLoader=new FileDownLoader();&#10;&#10;&#9;&#9;//&#31532;&#19968;&#23618;&#33719;&#21462;&#25152;&#26377;&#26495;&#22359;&#24086;&#23376;&#10;&#9;&#9;Set&#60;String&#62; boardsSet = boardParser.getBoards(seed);&#10;&#9;&#9;for (String visitUrl : boardsSet) &#123;&#10;&#9;&#9;&#9;String board = visitUrl.trim().split(&#34;=&#34;)[1];&#10;&#10;&#9;&#9;&#9;try &#123;&#10;&#9;&#9;&#9;&#9;Thread.sleep(500);&#10;&#9;&#9;&#9;&#125; catch (InterruptedException e) &#123;&#10;&#9;&#9;&#9;&#9;// TODO Auto-generated catch block&#10;&#9;&#9;&#9;&#9;e.printStackTrace();&#10;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;Map&#60;String, String&#62; fileMap = fileParser.getFileUrls(visitUrl);&#10;&#9;&#9;&#9;//&#31532;&#19977;&#23618;&#65292;&#19979;&#36733;&#27599;&#20010;&#24086;&#23376;&#30340;&#20869;&#23481;&#10;&#9;&#9;&#9;for (Entry&#60;String, String&#62; entry : fileMap.entrySet()) &#123;&#10;&#9;&#9;&#9;&#9;String id = entry.getKey().split(&#34;=&#34;)[2];&#10;&#9;&#9;&#9;&#9;String path = dir+&#34;/&#34;+board+&#34;/&#34;+entry.getValue().trim()+&#34;_&#34;+id+&#34;.txt&#34;;&#10;&#9;&#9;&#9;&#9;downLoader.downloadFile(entry.getKey(),path);&#10;&#9;&#9;&#9;&#9;try &#123;&#10;&#9;&#9;&#9;&#9;&#9;Thread.sleep(500);&#10;&#9;&#9;&#9;&#9;&#125; catch (InterruptedException e) &#123;&#10;&#9;&#9;&#9;&#9;&#9;// TODO Auto-generated catch block&#10;&#9;&#9;&#9;&#9;&#9;e.printStackTrace();&#10;&#9;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#9;&#125;&#10;&#9;&#9;&#125;&#10;&#9;&#125;&#10;&#9;//main &#26041;&#27861;&#20837;&#21475;&#10;&#9;public static void main(String[]args)&#10;&#9;&#123;&#10;&#9;&#9;Crawler crawler = new Crawler();&#10;&#9;&#9;crawler.crawling(&#34;http://bbs.nju.edu.cn/bbsall&#34;);&#10;&#9;&#125;&#10;&#125;</span><br></pre></td></tr></table></figure></p>
<hr>
<h2 id="代码上传到github">代码上传到github</h2><p>最后，因为htmlparser还会依赖一些其他的包，为了方便起见，已把代码发到<a href="https://github.com/rdmclin2/BBSCrawler" target="_blank" rel="external">github</a>上,希望能够帮到你。</p>
<hr>
<h1 id="参考">参考</h1><ul>
<li><a href="http://www.oschina.net/code/snippet_246199_9298" target="_blank" rel="external">htmlparser提取新闻</a></li>
<li><a href="http://gcgmh.iteye.com/blog/442560" target="_blank" rel="external">htmlparser精确提取</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-crawler/index.html" target="_blank" rel="external">使用 HttpClient 和 HtmlParser 实现简易爬虫</a></li>
</ul>
</div></article></div></section><footer><div class="paginator"><a href="/2015/08/19/html-note/" class="prev">上一篇</a><a href="/2015/08/10/github-pages-create-custrom-website/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'mcl';
var disqus_identifier = '2015/08/17/java-crawler-bbs-note/';
var disqus_title = 'Java爬虫爬取小百合BBS小记';
var disqus_url = 'http://www.mclspace.com/2015/08/17/java-crawler-bbs-note/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//mcl.disqus.com/count.js" async></script><div class="copyright"><p>© 2015 - 2016 <a href="http://www.mclspace.com">LinChen</a>, unless otherwise noted.</p></div></footer><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "//hm.baidu.com/hm.js?6b26d13232555e4d64b5a8c3567ccda9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>
<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="大猫Calvin"><title>Java爬虫爬取小百合BBS小记 · 坐在键盘上的猫</title><meta name="description" content="前言本文关于小百合论坛的爬取，共458个板块，每个板块1000个的帖子，主要借助开源包htmlparser,下面上分析过程和代码。注意本文在一篇ibm的文章的基础上进行修改。
更新我另外简单实现了一个MapReduce版本的，不过实现的不好，权当参考一下吧.奉上github地址，上面有简单介绍.

"><meta name="keywords" content="front end, nodejs, html,css,js,android,java"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="/images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/blog_basic.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"></head><body><div class="sidebar animated fadeInDown"><div class="logo-title"><div class="title"><img src="/images/logo@2x.png" style="width:127px;"><h3 title=""><a href="/">坐在键盘上的猫</a></h3><div class="description"><p>站在艺术与技术交汇之处</p></div></div></div><ul class="social-links"><li><a href="/atom.xml"><i class="fa fa-rss"></i></a></li><li><a href="http://weibo.com/rdmclin2"><i class="fa fa-weibo"></i></a></li></ul><div class="footer"><a target="_blank" href="/"><span>Theme by </span></a><a href="https://www.caicai.me"> CaiCai</a><span>&</span><a href="https://github.com/Ben02/hexo-theme-Anatole"> Ben</a><div class="by_farbox"><a href="https://hexo.io/zh-cn/" target="_blank">Proudly published with Hexo&#65281;</a></div></div></div><div class="main"><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/">首页</a></li><li><a href="/about">关于</a></li><li><a href="/archives">归档</a></li><li><a href="/links">友链</a></li></div><div class="information"><div class="back_btn"><li><a onclick="window.history.go(-1)" class="fa fa-chevron-left"> </a></li></div><div class="avatar"><img src="https://avatars2.githubusercontent.com/u/4705237?v=3&amp;s=460"></div></div></div><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Java爬虫爬取小百合BBS小记</a></h3></div><div class="post-content"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文关于<a href="http://bbs.nju.edu.cn" target="_blank" rel="external">小百合论坛</a>的爬取，共458个板块，每个板块1000个的帖子，主要借助开源包<a href="http://sourceforge.net/projects/htmlparser/files/Integration-Builds/2.0-20060923/" target="_blank" rel="external">htmlparser</a>,下面上分析过程和代码。注意本文在一篇<a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-crawler/index.html" target="_blank" rel="external">ibm的文章</a>的基础上进行修改。</p>
<h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><p>我另外简单实现了一个MapReduce版本的，不过实现的不好，权当参考一下吧.奉上<a href="git@github.com:rdmclin2/MR_BBSCrawler.git">github地址</a>，上面有简单介绍.</p>
<a id="more"></a>
<hr>
<h1 id="分析过程及代码"><a href="#分析过程及代码" class="headerlink" title="分析过程及代码"></a>分析过程及代码</h1><h2 id="关于爬虫"><a href="#关于爬虫" class="headerlink" title="关于爬虫"></a>关于爬虫</h2><p>爬虫大家都知道，由一个种子页(seed)出发，分析该页，获取页面链接，加入未访问的列表中，从未访问的表中取链接，访问，重复如上步骤即可，要注意的就是要维护一个已访问的网页列表，以避免重复访问。然而这种广义的爬虫并不符合特定的情况，例如我现在要爬的bbs板块帖子，遇到具体问题，还是得具体分析。</p>
<hr>
<h2 id="小百合bbs爬取过程"><a href="#小百合bbs爬取过程" class="headerlink" title="小百合bbs爬取过程"></a>小百合bbs爬取过程</h2><p>基本的爬取步骤如下：</p>
<ul>
<li>首先获得小百合bbs的所有板块链接</li>
<li>对于每一个板块，获取至多1000条帖子的链接</li>
<li>对于每一条链接，获取帖子内容并存储</li>
</ul>
<hr>
<h3 id="获取所有板块链接"><a href="#获取所有板块链接" class="headerlink" title="获取所有板块链接"></a>获取所有板块链接</h3><p>那么，最好有个入口页面可以把所有板块的链接都收集好，直接爬就好，幸运的是小百合贴心的有个叫做<a href="http://bbs.nju.edu.cn/bbsall" target="_blank" rel="external">全部讨论区</a>的版面,lucky~省下不少功夫.<br>这一部分的代码如下(BoardParser.java):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line">package com.mcl.crawler;</div><div class="line"></div><div class="line">import java.util.HashSet;</div><div class="line">import java.util.Set;</div><div class="line"></div><div class="line">import org.htmlparser.Node;</div><div class="line">import org.htmlparser.Parser;</div><div class="line">import org.htmlparser.filters.NodeClassFilter;</div><div class="line">import org.htmlparser.tags.LinkTag;</div><div class="line">import org.htmlparser.util.NodeList;</div><div class="line">import org.htmlparser.util.ParserException;</div><div class="line"></div><div class="line">public class BoardParser &#123;</div><div class="line">	// 获取bbs网站上的所有板块链接</div><div class="line">	private Set&lt;String&gt; extracLinks(String url, LinkFilter filter) &#123;</div><div class="line"></div><div class="line">		Set&lt;String&gt; links = new HashSet&lt;String&gt;();</div><div class="line">		try &#123;</div><div class="line">			Parser parser = new Parser(url);</div><div class="line">			parser.setEncoding(&quot;UTF-8&quot;);</div><div class="line">			// linkFilter 来设置过滤 &lt;a&gt; 标签</div><div class="line">			NodeClassFilter linkFilter = new NodeClassFilter(LinkTag.class);</div><div class="line">			// 得到所有经过过滤的标签</div><div class="line">			NodeList list = parser.extractAllNodesThatMatch(linkFilter);</div><div class="line">			for (int i = 0; i &lt; list.size(); i++) &#123;</div><div class="line">				Node tag = list.elementAt(i);</div><div class="line">				if (tag instanceof LinkTag)// &lt;a&gt; 标签</div><div class="line">				&#123;</div><div class="line">					LinkTag link = (LinkTag) tag;</div><div class="line">					String linkUrl = link.getLink();// url</div><div class="line">					if (filter.accept(linkUrl)) &#123;</div><div class="line">						// 将一般模式切换成主题模式</div><div class="line">						String stlink = linkUrl.replace(&quot;bbsdoc&quot;, &quot;bbstdoc&quot;);</div><div class="line">						links.add(stlink);</div><div class="line">					&#125;</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125; catch (ParserException e) &#123;</div><div class="line">			e.printStackTrace();</div><div class="line">		&#125;</div><div class="line">		return links;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	// 获取板块链接并切换成主题模式</div><div class="line">	public Set&lt;String&gt; getBoards(String seed) &#123;</div><div class="line">		Set&lt;String&gt; links = extracLinks(seed, new LinkFilter() &#123;</div><div class="line">			// 提取以 http://bbs.nju.edu.cn/bbsdoc 开头的链接</div><div class="line">			public boolean accept(String url) &#123;</div><div class="line">				if (url.startsWith(&quot;http://bbs.nju.edu.cn/bbsdoc&quot;))</div><div class="line">					return true;</div><div class="line">				else</div><div class="line">					return false;</div><div class="line">			&#125;</div><div class="line">		&#125;);</div><div class="line">		// System.out.println(links.size());</div><div class="line">		// for(String link : links)&#123;</div><div class="line">		// System.out.println(link);</div><div class="line">		// &#125;</div><div class="line">		return links;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	// 测试的 main 方法</div><div class="line">	// public static void main(String[]args)&#123;</div><div class="line">	// BoardParser.getBoards(&quot;http://bbs.nju.edu.cn/bbsall&quot;);</div><div class="line">	// &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="对于每一个板块，获取至多1000条帖子的链接"><a href="#对于每一个板块，获取至多1000条帖子的链接" class="headerlink" title="对于每一个板块，获取至多1000条帖子的链接"></a>对于每一个板块，获取至多1000条帖子的链接</h3><p>然后有了页面的链接，就要考虑怎么获取1000个帖子，我们并不想爬取回复，直接爬取主题，因此你可以看到上面将链接中的bbsdoc换成了bbstdoc，这就是主题模式。注意到bbs有个<code>上一页</code>的链接，那么获取这个链接，可以获得更多的帖子，不断反复，直到获取1000个帖子或没有更多的帖子.<br>以下是主要处理代码(FileParser.java):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div></pre></td><td class="code"><pre><div class="line">package com.mcl.crawler;</div><div class="line"></div><div class="line">import java.util.HashMap;</div><div class="line">import java.util.Map;</div><div class="line">import java.util.Map.Entry;</div><div class="line"></div><div class="line">import org.htmlparser.Node;</div><div class="line">import org.htmlparser.Parser;</div><div class="line">import org.htmlparser.filters.NodeClassFilter;</div><div class="line">import org.htmlparser.tags.LinkTag;</div><div class="line">import org.htmlparser.util.NodeList;</div><div class="line">import org.htmlparser.util.ParserException;</div><div class="line"></div><div class="line">//获取bbs网站上某板块的所有帖子链接</div><div class="line">public class FileParser &#123;</div><div class="line">	//每个板块爬取1000个左右的帖子</div><div class="line">	private final int LINK_UP = 1000;</div><div class="line"></div><div class="line">	Map&lt;String,String&gt; links = new HashMap&lt;String,String&gt;();</div><div class="line">	private  String extracLinks(String url,LinkFilter filter) &#123;</div><div class="line">		//下一页的链接</div><div class="line">		String nextPage = null;</div><div class="line">		try &#123;</div><div class="line">			Parser parser = new Parser(url);</div><div class="line">			parser.setEncoding(&quot;gb2312&quot;);</div><div class="line">			// linkFilter 来设置过滤 &lt;a&gt; 标签</div><div class="line">			NodeClassFilter linkFilter = new NodeClassFilter(</div><div class="line">					LinkTag.class);</div><div class="line">			// 得到所有经过过滤的标签</div><div class="line">			NodeList list = parser.extractAllNodesThatMatch(linkFilter);</div><div class="line">			for (int i = 0; i &lt; list.size(); i++) &#123;</div><div class="line">				Node tag = list.elementAt(i);</div><div class="line">				if (tag instanceof LinkTag)// &lt;a&gt; 标签</div><div class="line">				&#123;</div><div class="line">					LinkTag link = (LinkTag) tag;</div><div class="line">					String linkUrl = link.getLink();// url</div><div class="line"></div><div class="line">					//获取上一页的链接，作为下一步的链接</div><div class="line">					if(link.getLinkText().equals(&quot;上一页&quot;))&#123;</div><div class="line">						nextPage = linkUrl;</div><div class="line">					&#125;</div><div class="line"></div><div class="line">					if(filter.accept(linkUrl) &amp;&amp; links.size() &lt; LINK_UP)&#123;</div><div class="line">						String title = link.getLinkText();</div><div class="line">						links.put(linkUrl, title);</div><div class="line">					&#125;</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125; catch (ParserException e) &#123;</div><div class="line">			e.printStackTrace();</div><div class="line">		&#125;</div><div class="line">		return nextPage;</div><div class="line">	&#125;</div><div class="line"></div><div class="line"></div><div class="line">		//获取板块链接并切换成主题模式</div><div class="line">		public  Map&lt;String, String&gt; getFileUrls(String seed)</div><div class="line">		&#123;</div><div class="line">			System.out.println(&quot;seed: &quot;+ seed);</div><div class="line">			links.clear();</div><div class="line">			int linkCount = 0;</div><div class="line">			final String rule = seed.replace(&quot;bbstdoc&quot;, &quot;bbstcon&quot;);</div><div class="line">			String list = seed;</div><div class="line">			while(linkCount &lt; LINK_UP)&#123;</div><div class="line">				list = extracLinks(list,new LinkFilter()</div><div class="line">				&#123;</div><div class="line">					//提取以 http://bbs.nju.edu.cn/bbstcon?board=... 开头的链接</div><div class="line">					public boolean accept(String url) &#123;</div><div class="line">						if(url.startsWith(rule))</div><div class="line">							return true;</div><div class="line">						else</div><div class="line">							return false;</div><div class="line">					&#125;</div><div class="line">				&#125;);</div><div class="line">				linkCount = links.size();</div><div class="line">				//如果没有下一页的链接了</div><div class="line">				if(list == null)&#123;</div><div class="line">					break;</div><div class="line">				&#125;</div><div class="line">				try &#123;</div><div class="line">					Thread.sleep(360);</div><div class="line">				&#125; catch (InterruptedException e) &#123;</div><div class="line">					// TODO Auto-generated catch block</div><div class="line">					e.printStackTrace();</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">			System.out.println(&quot;共 &quot;+ links.size()+&quot; 篇帖子&quot;);</div><div class="line">			for (Entry&lt;String, String&gt; entry : links.entrySet()) &#123;</div><div class="line">				System.out.println(entry.getKey()+&quot; &quot;+entry.getValue());</div><div class="line">			&#125;</div><div class="line">			return links;</div><div class="line">		&#125;</div><div class="line"></div><div class="line">		//测试的 main 方法</div><div class="line">		public static void main(String[]args)&#123;</div><div class="line">			FileParser parser = new FileParser();</div><div class="line">			parser.getFileUrls(&quot;http://bbs.nju.edu.cn/bbstdoc?board=Blog&quot;);</div><div class="line">		&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="对于每一条链接，获取帖子内容并存储"><a href="#对于每一条链接，获取帖子内容并存储" class="headerlink" title="对于每一条链接，获取帖子内容并存储"></a>对于每一条链接，获取帖子内容并存储</h3><p>得到帖子的链接，下面就是获取帖子的内容,我们获取table-&gt;tbody-&gt;第二个tr-&gt;td-&gt;pre中的纯文本。<br>代码如下(FileDownLoader):<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div></pre></td><td class="code"><pre><div class="line">package com.mcl.crawler;</div><div class="line"></div><div class="line">import java.io.BufferedWriter;</div><div class="line">import java.io.File;</div><div class="line">import java.io.FileOutputStream;</div><div class="line">import java.io.IOException;</div><div class="line">import java.io.OutputStreamWriter;</div><div class="line"></div><div class="line">import org.htmlparser.NodeFilter;</div><div class="line">import org.htmlparser.Parser;</div><div class="line">import org.htmlparser.filters.AndFilter;</div><div class="line">import org.htmlparser.filters.HasAttributeFilter;</div><div class="line">import org.htmlparser.filters.NodeClassFilter;</div><div class="line">import org.htmlparser.filters.TagNameFilter;</div><div class="line">import org.htmlparser.tags.TableTag;</div><div class="line">import org.htmlparser.util.NodeList;</div><div class="line">import org.htmlparser.util.ParserException;</div><div class="line"></div><div class="line">public class FileDownLoader &#123;</div><div class="line">public static boolean createFile(String destFileName) &#123;  </div><div class="line">      File file = new File(destFileName);  </div><div class="line">      if(file.exists()) &#123;  </div><div class="line">          System.out.println(&quot;创建单个文件&quot; + destFileName + &quot;失败，目标文件已存在！&quot;);</div><div class="line">          return false;  </div><div class="line">      &#125;  </div><div class="line">      if (destFileName.endsWith(File.separator)) &#123;  </div><div class="line">          System.out.println(&quot;创建单个文件&quot; + destFileName + &quot;失败，目标文件不能为目录！&quot;);  </div><div class="line">          return false;  </div><div class="line">      &#125;  </div><div class="line">      //判断目标文件所在的目录是否存在  </div><div class="line">      if(!file.getParentFile().exists()) &#123;  </div><div class="line">          //如果目标文件所在的目录不存在，则创建父目录  </div><div class="line">          //System.out.println(&quot;目标文件所在目录不存在，准备创建它！&quot;);  </div><div class="line">          if(!file.getParentFile().mkdirs()) &#123;  </div><div class="line">              System.out.println(&quot;创建目标文件所在目录失败！&quot;);  </div><div class="line">              return false;  </div><div class="line">          &#125;  </div><div class="line">      &#125;  </div><div class="line">      //创建目标文件  </div><div class="line">      try &#123;  </div><div class="line">          if (file.createNewFile()) &#123;  </div><div class="line">              return true;  </div><div class="line">          &#125; else &#123;  </div><div class="line">              return false;  </div><div class="line">          &#125;  </div><div class="line">      &#125; catch (IOException e) &#123;  </div><div class="line">          e.printStackTrace();  </div><div class="line">          System.out.println(&quot;创建单个文件&quot; + destFileName + &quot;失败！&quot; + e.getMessage());  </div><div class="line">          return false;  </div><div class="line">      &#125;  </div><div class="line">  &#125;</div><div class="line"></div><div class="line"></div><div class="line">/**保存网页字节数组到本地文件</div><div class="line"> * filePath 为要保存的文件的相对地址</div><div class="line"> */</div><div class="line"> private void saveToLocal(String content,String filePath)</div><div class="line"> &#123;</div><div class="line">   try &#123;</div><div class="line">     createFile(filePath);</div><div class="line">     OutputStreamWriter osw;</div><div class="line">     String encoding = &quot;UTF-8&quot;;</div><div class="line">     osw = new OutputStreamWriter(</div><div class="line">         new FileOutputStream(filePath), encoding);</div><div class="line">     BufferedWriter bw = new BufferedWriter(osw);</div><div class="line">     bw.write(content);</div><div class="line"></div><div class="line">     bw.close();</div><div class="line">     osw.close();</div><div class="line">   &#125; catch (IOException e) &#123;</div><div class="line">     e.printStackTrace();</div><div class="line">   &#125;</div><div class="line"> &#125;</div><div class="line"></div><div class="line">/*下载 url 指向的网页的帖子内容并存储*/</div><div class="line">public String  downloadFile(String url,String filePath)</div><div class="line">&#123;</div><div class="line">  System.out.println(&quot;filePath: &quot;+filePath);</div><div class="line">  Parser parser;</div><div class="line">  try &#123;</div><div class="line">    parser = new Parser(url);</div><div class="line">    parser.setEncoding(&quot;UTF-8&quot;);</div><div class="line">    NodeList tableOfPre1 = parser.extractAllNodesThatMatch(</div><div class="line">          (NodeFilter) new AndFilter(new NodeClassFilter(TableTag.class), new HasAttributeFilter(&quot;class&quot;, &quot;main&quot;)));</div><div class="line">      if(tableOfPre1 != null &amp;&amp; tableOfPre1.size() &gt; 0) &#123;</div><div class="line">         // 获取指定 table 标签的子节点中的 &lt;tbody&gt; 节点</div><div class="line">         NodeList tList = tableOfPre1.elementAt(0).getChildren().extractAllNodesThatMatch (new TagNameFilter(&quot;tr&quot;), true);</div><div class="line">         //第二列第一个tag：pre</div><div class="line">         String text = tList.elementAt(1).getFirstChild().toPlainTextString();</div><div class="line">         //System.out.println(text);</div><div class="line">         saveToLocal(text,filePath);</div><div class="line"></div><div class="line">      &#125;</div><div class="line">    &#125; catch (ParserException e) &#123;</div><div class="line">      e.printStackTrace();</div><div class="line">    &#125;</div><div class="line">  return null;</div><div class="line">&#125;</div><div class="line">//测试的 main 方法</div><div class="line">public static void main(String[]args)</div><div class="line">&#123;</div><div class="line">  FileDownLoader downLoader = new FileDownLoader();</div><div class="line">  downLoader.downloadFile(&quot;http://bbs.nju.edu.cn/bbstcon?board=Blog&amp;file=M.1373384270.A&quot;,&quot;temp/Blog/我私人区最后一篇文章自动消失了，能找回吗？.txt&quot;);</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<h3 id="main文件"><a href="#main文件" class="headerlink" title="main文件"></a>main文件</h3><p>最后把这些步骤串起来,分为3层进行处理,因为帖子名称可能重复，因此在生成的文件名中加入帖子的id以保证唯一性,最后因为小百合连续获取帖子会出错，因此我们间隔一段时间获取数据，这个时间需要自己测试.<br>主要代码如下(Crawler.java)：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">package com.mcl.crawler;</div><div class="line"></div><div class="line">import java.util.Dictionary;</div><div class="line">import java.util.Map;</div><div class="line">import java.util.Set;</div><div class="line">import java.util.Map.Entry;</div><div class="line">public class Crawler &#123;</div><div class="line">	private final String dir = &quot;temp&quot;;</div><div class="line">	/* 爬取方法*/</div><div class="line">	public void crawling(String seed)</div><div class="line">	&#123;</div><div class="line">		FileParser fileParser = new FileParser();</div><div class="line">		BoardParser boardParser = new BoardParser();</div><div class="line">		FileDownLoader downLoader=new FileDownLoader();</div><div class="line"></div><div class="line">		//第一层获取所有板块帖子</div><div class="line">		Set&lt;String&gt; boardsSet = boardParser.getBoards(seed);</div><div class="line">		for (String visitUrl : boardsSet) &#123;</div><div class="line">			String board = visitUrl.trim().split(&quot;=&quot;)[1];</div><div class="line"></div><div class="line">			try &#123;</div><div class="line">				Thread.sleep(500);</div><div class="line">			&#125; catch (InterruptedException e) &#123;</div><div class="line">				// TODO Auto-generated catch block</div><div class="line">				e.printStackTrace();</div><div class="line">			&#125;</div><div class="line">			Map&lt;String, String&gt; fileMap = fileParser.getFileUrls(visitUrl);</div><div class="line">			//第三层，下载每个帖子的内容</div><div class="line">			for (Entry&lt;String, String&gt; entry : fileMap.entrySet()) &#123;</div><div class="line">				String id = entry.getKey().split(&quot;=&quot;)[2];</div><div class="line">				String path = dir+&quot;/&quot;+board+&quot;/&quot;+entry.getValue().trim()+&quot;_&quot;+id+&quot;.txt&quot;;</div><div class="line">				downLoader.downloadFile(entry.getKey(),path);</div><div class="line">				try &#123;</div><div class="line">					Thread.sleep(500);</div><div class="line">				&#125; catch (InterruptedException e) &#123;</div><div class="line">					// TODO Auto-generated catch block</div><div class="line">					e.printStackTrace();</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	//main 方法入口</div><div class="line">	public static void main(String[]args)</div><div class="line">	&#123;</div><div class="line">		Crawler crawler = new Crawler();</div><div class="line">		crawler.crawling(&quot;http://bbs.nju.edu.cn/bbsall&quot;);</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<hr>
<h2 id="代码上传到github"><a href="#代码上传到github" class="headerlink" title="代码上传到github"></a>代码上传到github</h2><p>最后，因为htmlparser还会依赖一些其他的包，为了方便起见，已把代码发到<a href="https://github.com/rdmclin2/BBSCrawler" target="_blank" rel="external">github</a>上,希望能够帮到你。</p>
<hr>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="http://www.oschina.net/code/snippet_246199_9298" target="_blank" rel="external">htmlparser提取新闻</a></li>
<li><a href="http://gcgmh.iteye.com/blog/442560" target="_blank" rel="external">htmlparser精确提取</a></li>
<li><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-crawler/index.html" target="_blank" rel="external">使用 HttpClient 和 HtmlParser 实现简易爬虫</a></li>
</ul>
</div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-sun-o"></i><span class="date">2015-08-17</span><i class="fa fa-comment-o"></i><a href="/2015/08/17/java-crawler-bbs-note/#comments">评论</a><i class="fa fa-tag"></i><a href="/categories/dev/" title="dev" class="tag">dev </a><a href="/tags/java/" title="java" class="tag">java </a><a href="/tags/crawler/" title="crawler" class="tag">crawler </a></div></div></div></div><div class="share"><div class="evernote"> <a href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank" class="fa fa-bookmark"></a></div><div class="weibo"> <a href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));" class="fa fa-weibo"></a></div><div class="twitter"> <a href="http://twitter.com/home?status=,http://www.mclspace.com/2015/08/17/java-crawler-bbs-note/,坐在键盘上的猫,Java爬虫爬取小百合BBS小记,;" class="fa fa-twitter"></a></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a role="navigation" href="/2015/08/19/html-note/" title="《Head First HTML 与 CSS》读书笔记之HTML篇" class="btn">上一篇</a></li><li class="next pagbuttons"><a role="navigation" href="/2015/08/10/github-pages-create-custrom-website/" title="用github pages创建自定义网站并绑定子域名" class="btn">下一篇</a></li></ul></div><a id="comments"></a><div data-thread-key="2015/08/17/java-crawler-bbs-note/" data-title="Java爬虫爬取小百合BBS小记" data-url="http://www.mclspace.com/2015/08/17/java-crawler-bbs-note/" data-author-key="1" class="ds-thread"></div><script>var duoshuoQuery = {short_name:"coco"};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
})();

</script></div></div></div></div><script src="/js/jquery.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script></body></html>
<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Mac上Hadoop的安装及使用 · Mcl's space</title><meta name="description" content="Mac上Hadoop的安装及使用 - LinChen"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="short icon" href="/favicon.ico"><link rel="stylesheet" href="/css/apollo.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,600" type="text/css"></head><body><header><a href="/" class="logo-link"><img src="/favicon.png"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="http://weibo.com/rdmclin2" target="_blank" class="nav-list-link">WEIBO</a></li><li class="nav-list-item"><a href="https://github.com/rdmclin2" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li><li class="nav-list-item"><a href="http://www.jianshu.com/users/34733105f724/latest_articles" target="_blank" class="nav-list-link">JIANSHU</a></li></ul></header><section class="container"><div class="post"><article class="post-block"><h1 class="post-title">Mac上Hadoop的安装及使用</h1><div class="post-time">Sep 1, 2015</div><div class="post-content"><blockquote>
<p>本文在mac osx 上安装配置hadoop环境,配置eclipse开发环境，并打包jar文件在集群上运行.</p>
</blockquote>
<a id="more"></a>
<h2 id="Mac安装hadoop环境"><code>Mac安装hadoop环境</code></h2><p>参考 <a href="http://andrewliu.in/2015/03/05/%E5%9C%A8Mac-OSX-Yosemite%E4%B8%8A%E5%AE%89%E8%A3%85Hadoop/" target="_blank" rel="external">在Mac OSX Yosemite上安装Hadoop</a></p>
<h3 id="设置alias">设置alias</h3><p>在zsh的.zshrc中配置打开和关闭hadoop的alias<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alias hstart=&#34;/usr/local/Cellar/hadoop/2.7.1/sbin/start-dfs.sh;/usr/local/Cellar/hadoop/2.7.1/sbin/start-yarn.sh&#34;&#10;alias hstop=&#34;/usr/local/Cellar/hadoop/2.7.1/sbin/stop-yarn.sh;/usr/local/Cellar/hadoop/2.7.1/sbin/stop-dfs.sh&#34;</span><br></pre></td></tr></table></figure></p>
<p>然后还有hbase的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alias hbstart=&#34;/usr/local/Cellar/hbase/1.0.1/libexec/bin/start-hbase.sh&#34;&#10;alias hbstop=&#34;/usr/local/Cellar/hbase/1.0.1/libexec/bin/stop-hbase.sh&#34;</span><br></pre></td></tr></table></figure></p>
<h3 id="开启成功的标识">开启成功的标识</h3><p>使用<code>jps</code>命令，出现以下几项:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2821 ResourceManager&#10;2582 DataNode&#10;2918 NodeManager&#10;2968 Jps&#10;2699 SecondaryNameNode&#10;2493 NameNode</span><br></pre></td></tr></table></figure></p>
<p>使用如下命令测试是否能够正确执行hadoop程序:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.1.jar pi 2 5</span><br></pre></td></tr></table></figure></p>
<h3 id="查看web任务的接口">查看web任务的接口</h3><p>我们可以通过以下web接口查看mapreduce任务<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:8088 Cluster Status &#36825;&#20010;&#25509;&#21475;&#38750;&#24120;&#26377;&#29992;,2.2&#20043;&#21518;,&#20043;&#21069;&#29992;50030&#31471;&#21475;&#10;http://localhost:50070/ HDFS status&#10;http://localhost:50090 secondaryNamenode</span><br></pre></td></tr></table></figure></p>
<hr>
<p><br></p>
<h2 id="安装配置eclipse插件"><code>安装配置eclipse插件</code></h2><h3 id="下载地址(2-7-0)">下载地址(2.7.0)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://download.csdn.net/detail/yew1eb/8716117</span><br></pre></td></tr></table></figure>
<h3 id="插件设置">插件设置</h3><p>在偏好设置 -&gt; Hadoop Map/Reduce中配置Hadoop installation directory为:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/Cellar/hadoop/2.7.1/libexec&#10;#&#22240;&#20154;&#32780;&#24322;,&#36825;&#37324;&#26159;&#29992;homebrew&#23433;&#35013;hadoop&#21518;&#30340;&#22320;&#22336;</span><br></pre></td></tr></table></figure></p>
<h3 id="打开mapreduce工作空间">打开mapreduce工作空间</h3><p>Window -&gt; Open Perspective -&gt; Other -&gt; Map/Reduce</p>
<h3 id="添加hadoop地址">添加hadoop地址</h3><p>在下面状态栏的Map/Reduce Locations中右击,选择New Hadoop location<br>配置 ：</p>
<ul>
<li>Location name : 随意,如<code>hadoop</code></li>
<li><p>Map/Reduce Master的端口参考</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop/mapred-site.xml</span><br></pre></td></tr></table></figure>
</li>
<li><p>DFS Master的端口参考</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/Cellar/hadoop/2.7.1/libexec/etc/hadoop/core-site.xml</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="配置完成标志">配置完成标志</h3><p>点击左侧的DFSLocations—&gt;hadoop（上一步配置的location name)，如能看到user，表示安装成功（保证已经启动了hadoop)</p>
<hr>
<p><br></p>
<h2 id="实战程序wordcount"><code>实战程序wordcount</code></h2><h3 id="新建WordCount项目">新建WordCount项目</h3><p>File—&gt;Project，选择Map/Reduce Project，输入项目名称WordCount等。</p>
<p>在WordCount项目里新建class，名称为WordCount，代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import java.io.IOException;&#10;import java.util.StringTokenizer;&#10;&#10;import org.apache.hadoop.conf.Configuration;&#10;import org.apache.hadoop.fs.Path;&#10;import org.apache.hadoop.io.IntWritable;&#10;import org.apache.hadoop.io.Text;&#10;import org.apache.hadoop.mapreduce.Job;&#10;import org.apache.hadoop.mapreduce.Mapper;&#10;import org.apache.hadoop.mapreduce.Reducer;&#10;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;&#10;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;&#10;import org.apache.hadoop.util.GenericOptionsParser;&#10;&#10;public class WordCount &#123;&#10;&#10;public static class TokenizerMapper extends Mapper&#60;Object, Text, Text, IntWritable&#62;&#123;&#10;&#12288;&#12288;private final static IntWritable one = new IntWritable(1);&#10;&#12288;&#12288;private Text word = new Text();&#10;&#10;&#12288;&#12288;public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123;&#10;&#12288;&#12288;&#12288;&#12288;StringTokenizer itr = new StringTokenizer(value.toString());&#10;&#12288;&#12288;&#12288;&#12288;&#12288;&#12288;while (itr.hasMoreTokens()) &#123;&#10;&#12288;&#12288;&#12288;&#12288;&#12288;&#12288;&#12288;&#12288;word.set(itr.nextToken());&#10;&#12288;&#12288;&#12288;&#12288;&#12288;&#12288;&#12288;&#12288;context.write(word, one);&#10;&#12288;&#12288;&#12288;&#12288;&#12288;&#12288;&#125;&#10;&#12288;&#12288;&#125;&#10;&#125;&#10;&#10;public static class IntSumReducer extends Reducer&#60;Text,IntWritable,Text,IntWritable&#62; &#123;&#10;&#12288;&#12288;private IntWritable result = new IntWritable();&#10;&#12288;&#12288;public void reduce(Text key, Iterable&#60;IntWritable&#62; values,Context context) throws IOException, InterruptedException &#123;&#10;&#12288;&#12288;&#12288;&#12288;int sum = 0;&#10;&#12288;&#12288;&#12288;&#12288;for (IntWritable val : values) &#123;&#10;&#12288;&#12288;&#12288;&#12288;&#12288;&#12288;sum += val.get();&#10;&#12288;&#12288;&#12288;&#12288;&#125;&#10;&#12288;&#12288;&#12288;&#12288;result.set(sum);&#10;&#12288;&#12288;&#12288;&#12288;context.write(key, result);&#10;&#12288;&#12288;&#125;&#10;&#125;&#10;&#10;public static void main(String[] args) throws Exception &#123;&#10;&#12288;&#12288;Configuration conf = new Configuration();&#10;&#12288;&#12288;String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();&#10;&#12288;&#12288;if (otherArgs.length != 2) &#123;&#10;&#12288;&#12288;&#12288;&#12288;System.err.println(&#34;Usage: wordcount &#60;in&#62; &#60;out&#62;&#34;);&#10;&#12288;&#12288;&#12288;&#12288;System.exit(2);&#10;&#12288;&#12288;&#125;&#10;&#12288;&#12288;Job job = new Job(conf, &#34;word count&#34;);&#10;&#12288;&#12288;job.setJarByClass(WordCount.class);&#10;&#12288;&#12288;job.setMapperClass(TokenizerMapper.class);&#10;&#12288;&#12288;job.setCombinerClass(IntSumReducer.class);&#10;&#12288;&#12288;job.setReducerClass(IntSumReducer.class);&#10;&#12288;&#12288;job.setOutputKeyClass(Text.class);&#10;&#12288;&#12288;job.setOutputValueClass(IntWritable.class);&#10;&#12288;&#12288;FileInputFormat.addInputPath(job, new Path(otherArgs[0]));&#10;&#12288;&#12288;FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));&#10;&#12288;&#12288;System.exit(job.waitForCompletion(true) ? 0 : 1);&#10;&#125;&#10;&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="创建输入input目录">创建输入input目录</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /user&#10;hadoop fs -mkdir /user/input</span><br></pre></td></tr></table></figure>
<h3 id="拷贝本地README-txt到HDFS的input里">拷贝本地README.txt到HDFS的input里</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyFromLocal /opt/hadoop/README.txt /user/input</span><br></pre></td></tr></table></figure>
<h3 id="参数配置">参数配置</h3><p>点击WordCount.java，右键，点击Run As—&gt;Run Configurations，配置运行参数Arguments，即输入和输出文件夹(事先不能存在)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#12288;&#12288;hdfs://localhost:9000/user/input    hdfs://localhost:9000/user/output</span><br></pre></td></tr></table></figure></p>
<p>然后点击运行</p>
<h3 id="查看结果">查看结果</h3><p><code>hadoop fs -ls output</code>,可以看到有两个输出结果<code>_SUCCESS</code>和<code>part-r-00000</code><br>执行<code>hadoop fs -cat output/*</code></p>
<p>或者也可以展开DFS Locations，双击打开part-r00000查看结果</p>
<hr>
<p><br></p>
<h2 id="打包eclipse程序在集群上运行"><code>打包eclipse程序在集群上运行</code></h2><p>直接运行eclipse程序，访问localhost:8088窗口并不会显示正在运行的job，因为在eclipse上并没有配置集群环境，而是本地环境。</p>
<h3 id="打包jar">打包jar</h3><p>右击工程 -&gt; export… -&gt; java -&gt; jar file -&gt; next -&gt; 填写jar包位置 -&gt; finish</p>
<h3 id="运行">运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar crawler.jar MR_Crawler hdfs://localhost:9000/user/input/urls hdfs://localhost:9000/user/output&#10;# &#27880;&#24847;&#29992;MR_Crawler&#26159;&#22240;&#20026;&#27809;&#26377;&#37197;&#32622;&#20855;&#20307;&#30340;package&#65292;&#22914;&#26524;&#26377;&#21253;&#35201;&#21152;&#19978;&#21253;&#21517;.</span><br></pre></td></tr></table></figure>
<hr>
<p><br></p>
<h2 id="注意点"><code>注意点</code></h2><ul>
<li>运行后台程序前, 必须格式化新安装的HDFS, 并通过创建存储目录和初始化元数据创新空的文件系统, 执行下面命令<code>hdfs namenode -format</code></li>
<li>输出文件默认不能存在</li>
</ul>
</div></article></div></section><footer><div class="paginator"><a href="/2015/09/16/docker-file-note/" class="prev">上一篇</a><a href="/2015/08/22/docker-create-nodejs-mongodb-image/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'mcl';
var disqus_identifier = '2015/09/01/hadoop-note/';
var disqus_title = 'Mac上Hadoop的安装及使用';
var disqus_url = 'http://www.mclspace.com/2015/09/01/hadoop-note/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//mcl.disqus.com/count.js" async></script><div class="copyright"><p>© 2015 - 2016 <a href="http://www.mclspace.com">LinChen</a>, unless otherwise noted.</p></div></footer><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "//hm.baidu.com/hm.js?6b26d13232555e4d64b5a8c3567ccda9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script src="https://cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>